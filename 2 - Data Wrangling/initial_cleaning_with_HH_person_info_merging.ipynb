{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files into DataFrames\n",
    "df_per = pd.read_csv(\"Deliv_PER.csv\")\n",
    "df_hh = pd.read_csv(\"Deliv_HH.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SAMPN', 'RECMODE', 'RETMODE', 'INCEN', 'ILANG', 'CTFIP', 'AREA', 'STRATA', 'STYPE', 'CEC', 'GTYPE', 'GFLAG', 'RIBUS', 'HHVEH', 'HHBIC', 'VEHNEW', 'BUYER1', 'BUYER2', 'BUYER3', 'BUYER4', 'BUYER5', 'BUYER6', 'BUYER7', 'BUYER8', 'RESTY', 'O_RESTY', 'OWN', 'O_OWN', 'TEN', 'PREVCITY', 'PREVSTAT', 'PREVZIP', 'PHLNS', 'INCOM', 'HHSIZ', 'NONRELAT', 'HHEMP', 'HHSTU', 'HHLIC', 'RECDate', 'ASSN', 'DOW', 'HTRIPS', 'HCITY', 'HSTAT', 'HZIP', 'HHNOV1', 'HHNOV2', 'HHNOV3', 'HHNOV4', 'HHNOV5', 'HHNOV6', 'HHNOV7', 'HHNOV8', 'VEHOP', 'VEHPR', 'VEDTE', 'CMPLD', 'LDPER1', 'LDPER2', 'LDPER3', 'LDPER4', 'LDPER5', 'LDPER6', 'LDPER7', 'LDPER8', 'LDTRP', 'LDFlag', 'HPFlag', 'HH_Complete', 'GPS_Complete', 'HCTRACT', 'HPrimaryCity', 'HSTFIP', 'MTC_FInalFlag', 'HHWGT', 'EXPHHWGT', 'GPS_MTC']\n",
      "['ID', 'SAMPN', 'PERNO', 'RELAT', 'GEND', 'AGE', 'AGEB', 'HISP', 'RACE1', 'RACE2', 'RACE3', 'RACE4', 'O_RACE', 'NTVTY', 'CNTRY', 'LIC', 'USER', 'TRANS', 'TPTYP1', 'TPTYP2', 'TPTYP3', 'TPTYP4', 'TPTYP5', 'TPTYP6', 'TPTYP7', 'O_TPTYP', 'CLIP1', 'CLIP2', 'CLIP3', 'COMP', 'MET', 'PASSTL', 'FLEX', 'EMPLY', 'WKSTAT', 'O_WKSTAT', 'JOBS', 'WLOC', 'WNAME', 'WCITY', 'WSTAT', 'WZIP', 'WXST1', 'WXST2', 'WXCORD', 'WYCORD', 'WDAYS', 'WDAY1', 'WDAY2', 'WDAY3', 'WDAY4', 'WDAY5', 'WDAY6', 'WDAY7', 'HOURS', 'WSCHED', 'COMPR', 'WMODE', 'INDUS', 'O_INDUS', 'OCCUP', 'O_OCCUP', 'WLOC2', 'WNAME2', 'WCITY2', 'WSTAT2', 'WZIP2', 'WXST2_1', 'WXST2_2', 'WDAYS2', 'DISAB', 'DTYPE1', 'DTYPE2', 'DTYPE3', 'DTYPE4', 'DTYPE5', 'DTYPE6', 'DTYPE7', 'O_DTYPE', 'DSLIC', 'EDIS', 'TTRIP', 'TRNSUB', 'SUBAMT', 'SUBUNT', 'O_SUBUNT', 'WTRIP', 'BTRIP', 'STUDE', 'SCHOL', 'O_SCHOL', 'SLOC', 'SONLN', 'PRESCH', 'O_PRESCH', 'SNAME', 'SCITY', 'SSTAT', 'SZIP', 'SXST1', 'SXST2', 'SXCORD', 'SYCORD', 'SMODE', 'EDUCA', 'O_EDUCA', 'INTRV', 'PROXY', 'CMPLG', 'HVLOG', 'PTRIPS', 'TOLLF', 'TOLLR1', 'TOLLR2', 'TOLLR3', 'TOLLR4', 'TOLLR5', 'TOLLR6', 'TOLLR7', 'TOLLR8', 'TOLLR9', 'TOLLR10', 'TOLLB1', 'TOLLB2', 'TOLLB3', 'TOLLB4', 'TOLLB5', 'TOLLB6', 'TOLLB7', 'TOLLB8', 'TOLLB9', 'TOLLB10', 'HOVL', 'NOGOWHY', 'NOGOWHY_O', 'InComplete', 'Moto_trip', 'WCTFIP', 'WTRACT', 'SCTFIP', 'STRACT', 'WPrimaryCity', 'WSTFIP', 'W2PrimaryCity', 'W2STFIP', 'SPrimaryCity', 'SSTFIP', 'PERWGT', 'EXPPERWGT']\n"
     ]
    }
   ],
   "source": [
    "print(df_hh.columns.tolist())\n",
    "print(df_per.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_missing_values(full_df, missing_value_threshold=80):\n",
    "    \"\"\"\n",
    "    Drops columns from a DataFrame that have more than a specified percentage of missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_df (pd.DataFrame): The input DataFrame.\n",
    "    - missing_value_threshold (float): The percentage threshold for missing values above which columns will be dropped (default is 99.9%).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the columns with excessive missing values removed.\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentage = full_df.isnull().mean() * 100\n",
    "    \n",
    "    # Identify columns with more missing values than the threshold\n",
    "    columns_to_drop = missing_percentage[missing_percentage > missing_value_threshold].index.tolist()\n",
    "    print(columns_to_drop)\n",
    "    # Drop the identified columns and return the modified DataFrame\n",
    "    return full_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def categorize_and_merge_by_prefix_with_number(df):\n",
    "    # Create a dictionary to store merged columns\n",
    "    merged_columns = {}\n",
    "\n",
    "    # Initialize a dictionary to group columns by the prefix (first 4 characters)\n",
    "    column_groups = {}\n",
    "\n",
    "    # Loop through the column names and group them based on their prefix and numeric suffix\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Check if the column name starts with 4 or more characters and ends with a number\n",
    "            match = re.match(r'^([A-Za-z]{4,})(\\d+)$', col)\n",
    "            if match:\n",
    "                prefix = match.group(1)  # The prefix (first 4 or more characters)\n",
    "                if prefix not in column_groups:\n",
    "                    column_groups[prefix] = []\n",
    "                column_groups[prefix].append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {col}: {e}\")\n",
    "    \n",
    "    # Merge the grouped columns into new columns as lists\n",
    "    for prefix, columns in column_groups.items():\n",
    "        if len(columns) > 1:  # Only merge if there is more than one column with the same prefix\n",
    "            merged_columns[f'Merged_{prefix}'] = df[columns].apply(lambda row: row.tolist(), axis=1)\n",
    "            # Drop the original columns after merging\n",
    "            df.drop(columns=columns, inplace=True)\n",
    "    # Add the merged columns to the DataFrame\n",
    "    df = df.assign(**merged_columns)\n",
    "\n",
    "    return df, column_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_starting_with_LD(df):\n",
    "    \"\"\"\n",
    "    Drops columns in the dataframe whose names start with 'LD'.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe from which to drop columns.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated dataframe with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Identify columns that start with 'LD'\n",
    "    columns_to_drop = [col for col in df.columns if col.startswith('LD')]\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_specific_columns(df):\n",
    "    \"\"\"\n",
    "    Drops specific columns from the DataFrame if they exist. These are out of scope \n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame from which columns will be dropped.\n",
    "    - columns_to_drop (list): List of column names to drop.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    columns_to_drop = [\n",
    "    'VEHNO', 'O_PRKTY', 'O_TRANSYS', 'TripDistanceFlag', 'AirTripDistance', 'CTFIP', 'TRACT', 'PSTFIP', 'PERWGT', \n",
    "    'EXPPERWGT', 'RECMODE', 'RETMODE', 'STYPE', 'RIBUS', 'TEN', 'PREVCITY', 'PREVSTAT', 'PREVZIP', 'PHLNS', 'VEDTE', \n",
    "    'CMPLD', 'HH_Complete', 'HSTFIP', 'MTC_FInalFlag', 'ID', 'TPTYP1', 'TPTYP2', 'TPTYP3', 'TPTYP4', 'TPTYP5', 'TPTYP6', \n",
    "    'TPTYP7', 'O_TPTYP', 'CLIP1', 'CLIP2', 'CLIP3', 'COMP', 'MET', 'PASSTL', 'WNAME', 'WCITY', 'WSTAT', 'WXCORD', \n",
    "    'WYCORD', 'DTYPE1', 'DTYPE2', 'DTYPE3', 'DTYPE4', 'DTYPE5', 'DTYPE6', 'DTYPE7', 'INTRV', 'CMPLG', 'HVLOG', 'WCTFIP', \n",
    "    'WTRACT', 'SCTFIP', 'STRACT', 'WPrimaryCity', 'WSTFIP', 'W2PrimaryCity', 'W2STFIP', 'SPrimaryCity', 'SSTFIP', 'BODY', \n",
    "    'O_BODY', 'FUELT1', 'FUELT2', 'FUELT3', 'FUELT4', 'FUELT5', 'FUELT6', 'O_FUELT', 'CIGLT', 'VEHAQ', 'VEHOWN', 'O_VEHOWN', \n",
    "    'VEHINS', 'VEHOBD', 'VEHTRN', 'VEHDRT', 'O_VEHDRT', 'VEHCYL', 'O_VEHCYL', 'VEHOUT', 'VEHVLT', 'WYCNTV', 'O_WYCNTV', \n",
    "    'PRESCH', 'O_PRESCH', 'SNAME'\n",
    "    ]\n",
    "    # Drop columns that are in the list and exist in the DataFrame\n",
    "    columns_to_drop_in_df = [col for col in columns_to_drop if col in df.columns]\n",
    "    print(columns_to_drop_in_df)\n",
    "    # Drop the identified columns\n",
    "    df_dropped = df.drop(columns=columns_to_drop_in_df)\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['CTFIP', 'RECMODE', 'RETMODE', 'STYPE', 'RIBUS', 'TEN', 'PREVCITY', 'PREVSTAT', 'PREVZIP', 'PHLNS', 'CMPLD', 'HH_Complete', 'HSTFIP', 'MTC_FInalFlag']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_hh, column_groups = categorize_and_merge_by_prefix_with_number(df_hh)\n",
    "df_hh = drop_columns_starting_with_LD(df_hh)\n",
    "df_hh = drop_columns_with_missing_values(df_hh)\n",
    "df_hh = drop_specific_columns(df_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['PERWGT', 'EXPPERWGT', 'ID', 'PASSTL', 'WNAME', 'WCITY', 'WSTAT', 'WXCORD', 'WYCORD', 'INTRV', 'CMPLG', 'HVLOG', 'WCTFIP', 'WTRACT', 'SCTFIP', 'STRACT', 'WPrimaryCity', 'WSTFIP', 'SPrimaryCity', 'SSTFIP', 'SNAME']\n"
     ]
    }
   ],
   "source": [
    "#merge first \n",
    "#then drop and lastly cremove missins \n",
    "df_per, column_groups = categorize_and_merge_by_prefix_with_number(df_per)\n",
    "df_per = drop_columns_starting_with_LD(df_per)\n",
    "df_per = drop_columns_with_missing_values(df_per)\n",
    "df_per = drop_specific_columns(df_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SAMPN', 'RECMODE', 'RETMODE', 'INCEN', 'ILANG', 'CTFIP', 'AREA', 'STRATA', 'STYPE', 'GTYPE', 'RIBUS', 'HHVEH', 'HHBIC', 'VEHNEW', 'RESTY', 'OWN', 'TEN', 'PREVCITY', 'PREVSTAT', 'PREVZIP', 'PHLNS', 'INCOM', 'HHSIZ', 'HHEMP', 'HHSTU', 'HHLIC', 'RECDate', 'ASSN', 'DOW', 'HTRIPS', 'HCITY', 'HSTAT', 'HZIP', 'VEHOP', 'CMPLD', 'HPFlag', 'HH_Complete', 'HCTRACT', 'HPrimaryCity', 'HSTFIP', 'MTC_FInalFlag', 'HHWGT', 'EXPHHWGT', 'Merged_BUYER', 'Merged_HHNOV', 'Merged_LDPER']\n",
      "['ID', 'SAMPN', 'PERNO', 'RELAT', 'GEND', 'AGE', 'HISP', 'NTVTY', 'LIC', 'USER', 'TRANS', 'PASSTL', 'FLEX', 'EMPLY', 'WKSTAT', 'JOBS', 'WLOC', 'WNAME', 'WCITY', 'WSTAT', 'WZIP', 'WXCORD', 'WYCORD', 'WDAYS', 'HOURS', 'WSCHED', 'COMPR', 'WMODE', 'INDUS', 'OCCUP', 'DISAB', 'TTRIP', 'TRNSUB', 'WTRIP', 'BTRIP', 'STUDE', 'SCHOL', 'SNAME', 'SCITY', 'SSTAT', 'SZIP', 'SXCORD', 'SYCORD', 'SMODE', 'EDUCA', 'INTRV', 'CMPLG', 'HVLOG', 'PTRIPS', 'TOLLF', 'HOVL', 'NOGOWHY', 'Moto_trip', 'WCTFIP', 'WTRACT', 'SCTFIP', 'STRACT', 'WPrimaryCity', 'WSTFIP', 'SPrimaryCity', 'SSTFIP', 'PERWGT', 'EXPPERWGT', 'Merged_RACE', 'Merged_TPTYP', 'Merged_CLIP', 'Merged_WXST', 'Merged_WDAY', 'Merged_DTYPE', 'Merged_SXST', 'Merged_TOLLR', 'Merged_TOLLB']\n"
     ]
    }
   ],
   "source": [
    "print(df_hh.columns.tolist())\n",
    "print(df_per.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SAMPN  PERNO  RELAT  GEND   AGE  HISP  NTVTY  LIC  USER  TRANS  ...  \\\n",
      "0  1046924.0    1.0    1.0   2.0  77.0   2.0    1.0  1.0   1.0    2.0  ...   \n",
      "1  1046924.0    2.0    2.0   1.0  77.0   2.0    1.0  1.0   1.0    2.0  ...   \n",
      "2  1047092.0    1.0    1.0   2.0  50.0   2.0    1.0  1.0   1.0    2.0  ...   \n",
      "3  1047092.0    2.0    2.0   1.0  51.0   2.0    1.0  1.0   2.0    2.0  ...   \n",
      "4  1048704.0    1.0    1.0   2.0  36.0   1.0    2.0  2.0   NaN    2.0  ...   \n",
      "\n",
      "    HZIP  VEHOP  HPFlag  HCTRACT  HPrimaryCity     HHWGT    EXPHHWGT  \\\n",
      "0  92061    2.0       2    19101  PAUMA VALLEY  2.134283  625.389658   \n",
      "1  92061    2.0       2    19101  PAUMA VALLEY  2.134283  625.389658   \n",
      "2  94501    4.0       2   427700       ALAMEDA  0.784909  229.994688   \n",
      "3  94501    4.0       2   427700       ALAMEDA  0.784909  229.994688   \n",
      "4  91763    NaN       1      207     MONTCLAIR  1.853113  543.001077   \n",
      "\n",
      "                               Merged_BUYER  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "2  [2.0, nan, nan, nan, nan, nan, nan, nan]   \n",
      "3  [2.0, nan, nan, nan, nan, nan, nan, nan]   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "\n",
      "                               Merged_HHNOV  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "2  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "4  [2.0, nan, nan, nan, nan, nan, nan, nan]   \n",
      "\n",
      "                                Merged_LDPER  \n",
      "0   [nan, nan, nan, nan, nan, nan, nan, nan]  \n",
      "1   [nan, nan, nan, nan, nan, nan, nan, nan]  \n",
      "2  [98.0, nan, nan, nan, nan, nan, nan, nan]  \n",
      "3  [98.0, nan, nan, nan, nan, nan, nan, nan]  \n",
      "4   [nan, nan, nan, nan, nan, nan, nan, nan]  \n",
      "\n",
      "[5 rows x 82 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames on the \"SAMPN\" column\n",
    "merged_df = pd.merge(df_per, df_hh, on=\"SAMPN\", how=\"left\")\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(merged_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new file\n",
    "merged_df.to_csv(\"Merged_Deliv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

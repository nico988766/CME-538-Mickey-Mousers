{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65466183-9640-4ca8-9349-eb7a7665cae2",
   "metadata": {},
   "source": [
    "CME538 - Introduction to Data Science\n",
    "Big Project \n",
    "\n",
    "The purpose of the following code is to extract the data from a MS AccessData Base into smaller CSVs for their differnt folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2a96b0b-54fa-4683-874b-41a2475bd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Deliv_PLACE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\3016661164.py:36: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, access_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Deliv_ACTIVITY...\n",
      "Processing Deliv_HH...\n",
      "Processing Deliv_LD...\n",
      "Processing Deliv_PER...\n",
      "Processing Deliv_HH...\n",
      "Processing Deliv_VEH...\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your Access database\n",
    "db_path = r\"C:\\Users\\17052\\Downloads\\chts2013_caltrans_raw_survey\\CHTS_raw_caltrans_db.accdb\"\n",
    "\n",
    "# Connection string\n",
    "conn_str = (\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "    r'DBQ=' + db_path + ';'\n",
    ")\n",
    "\n",
    "# Connect to MS Access\n",
    "access_conn = pyodbc.connect(conn_str)\n",
    "access_cursor = access_conn.cursor()\n",
    "\n",
    "#for filetering students and Workers\n",
    "sample_numbers_df = pd.read_csv('studentworkerSAMPN.csv')\n",
    "sample_numbers_list = sample_numbers_df['0'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# List of tables to export\n",
    "\n",
    "tables = ['Deliv_PLACE','ASSN_TravelDate', 'Deliv_ACTIVITY', 'Deliv_HH', 'Deliv_LD', 'Deliv_PER', 'Deliv_HH', 'Deliv_VEH']  \n",
    "tables = ['Deliv_PLACE', 'Deliv_ACTIVITY', 'Deliv_HH', 'Deliv_LD', 'Deliv_PER', 'Deliv_HH', 'Deliv_VEH']  \n",
    "combined_df = None\n",
    "des_samp = ['1031985','1032036','1032053','1032425', '1032558']\n",
    "filtered_dfs = []\n",
    "# Loop through each table and export data to CSV\n",
    "for table in tables:\n",
    "    print(f\"Processing {table}...\")\n",
    "\n",
    "    # Query to select all data from the current table\n",
    "    query = f\"SELECT * FROM {table}\"\n",
    "    df = pd.read_sql(query, access_conn)\n",
    "    filtered_df = df[df['SAMPN'].isin(sample_numbers_list)]\n",
    "\n",
    "\n",
    "    \n",
    "    #filtered_df['orgin sheet'] = table\n",
    "    #filtered_dfs.append(df)\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate all filtered DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "# Close the connection\n",
    "access_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94431ef6-f994-4b29-a36a-8108b0f236e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "0      1047092.0\n",
      "1      1048704.0\n",
      "2      1048730.0\n",
      "3      1048736.0\n",
      "4      1048852.0\n",
      "...          ...\n",
      "33306  7212027.0\n",
      "33307  7212128.0\n",
      "33308  7212273.0\n",
      "33309  7212324.0\n",
      "33310  7212388.0\n",
      "\n",
      "[33311 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sample_numbers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d52b8c0a-a63a-4bf1-9b92-ae1bd477a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SAMPN  PERNO  PLANO  TOTTR  HHMEM  PER1  PER2  PER3  PER4  PER5  \\\n",
      "0        1032036.0    1.0    1.0    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1        1032036.0    1.0    2.0    3.0    2.0   4.0   5.0   NaN   NaN   NaN   \n",
      "2        1032036.0    1.0    3.0    5.0    4.0   2.0   3.0   4.0   5.0   NaN   \n",
      "3        1032036.0    1.0    4.0    2.0    1.0   2.0   NaN   NaN   NaN   NaN   \n",
      "4        1032036.0    1.0    5.0    2.0    1.0   2.0   NaN   NaN   NaN   NaN   \n",
      "...            ...    ...    ...    ...    ...   ...   ...   ...   ...   ...   \n",
      "1210378  7212273.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210379  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210380  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210381  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210382  7212388.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "         ...  VEHDRT  O_VEHDRT  VEHCYL  O_VEHCYL  VEHOUT VEHVLT VEHT  CNTV  \\\n",
      "0        ...     NaN       NaN     NaN       NaN     NaN    NaN  NaN   NaN   \n",
      "1        ...     NaN       NaN     NaN       NaN     NaN    NaN  NaN   NaN   \n",
      "2        ...     NaN       NaN     NaN       NaN     NaN    NaN  NaN   NaN   \n",
      "3        ...     NaN       NaN     NaN       NaN     NaN    NaN  NaN   NaN   \n",
      "4        ...     NaN       NaN     NaN       NaN     NaN    NaN  NaN   NaN   \n",
      "...      ...     ...       ...     ...       ...     ...    ...  ...   ...   \n",
      "1210378  ...     3.0      None     5.0      None     NaN    NaN  2.0   2.0   \n",
      "1210379  ...     3.0      None     2.0      None     NaN    NaN  2.0   2.0   \n",
      "1210380  ...     3.0      None     5.0      None     NaN    NaN  2.0   2.0   \n",
      "1210381  ...     1.0      None     4.0      None     NaN    NaN  2.0   2.0   \n",
      "1210382  ...     3.0      None     4.0      None     NaN    NaN  2.0   2.0   \n",
      "\n",
      "         WYCNTV O_WYCNTV  \n",
      "0           NaN      NaN  \n",
      "1           NaN      NaN  \n",
      "2           NaN      NaN  \n",
      "3           NaN      NaN  \n",
      "4           NaN      NaN  \n",
      "...         ...      ...  \n",
      "1210378     1.0     None  \n",
      "1210379    98.0     None  \n",
      "1210380    98.0     None  \n",
      "1210381    98.0     None  \n",
      "1210382     5.0     None  \n",
      "\n",
      "[1210383 rows x 362 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ada0b12f-c69c-4c2e-a1d6-11ff9eddc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\808201354.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['BUYERT'] = combined_df[['BUYER1', 'BUYER2', 'BUYER3', 'BUYER4', 'BUYER5', 'BUYER6', 'BUYER7']].values.tolist()\n",
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\808201354.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['HHNOVT'] = combined_df[['HHNOV1', 'HHNOV2', 'HHNOV3', 'HHNOV4', 'HHNOV5', 'HHNOV6', 'HHNOV7', 'HHNOV8']].values.tolist()\n",
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\808201354.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['LDPERT'] = combined_df[['LDPER1', 'LDPER2', 'LDPER3', 'LDPER4', 'LDPER5', 'LDPER6', 'LDPER7', 'LDPER8',]].values.tolist()\n",
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\808201354.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['TOOLRT'] = combined_df[['TOLLR1', 'TOLLR2', 'TOLLR3', 'TOLLR4', 'TOLLR5', 'TOLLR6', 'TOLLR7', 'TOLLR8', 'TOLLR9', 'TOLLR10']].values.tolist()\n",
      "C:\\Users\\17052\\AppData\\Local\\Temp\\ipykernel_2116\\808201354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['TOOLBT'] = combined_df[['TOLLB1', 'TOLLB2', 'TOLLB3', 'TOLLB4', 'TOLLB5', 'TOLLB6', 'TOLLB7', 'TOLLB8', 'TOLLB9', 'TOLLB10']].values.tolist()\n"
     ]
    }
   ],
   "source": [
    "#merge some of the columns that all hold catagorcal data \n",
    "combined_df['BUYERT'] = combined_df[['BUYER1', 'BUYER2', 'BUYER3', 'BUYER4', 'BUYER5', 'BUYER6', 'BUYER7']].values.tolist()\n",
    "combined_df['HHNOVT'] = combined_df[['HHNOV1', 'HHNOV2', 'HHNOV3', 'HHNOV4', 'HHNOV5', 'HHNOV6', 'HHNOV7', 'HHNOV8']].values.tolist()\n",
    "combined_df['LDPERT'] = combined_df[['LDPER1', 'LDPER2', 'LDPER3', 'LDPER4', 'LDPER5', 'LDPER6', 'LDPER7', 'LDPER8',]].values.tolist()\n",
    "combined_df['TOOLRT'] = combined_df[['TOLLR1', 'TOLLR2', 'TOLLR3', 'TOLLR4', 'TOLLR5', 'TOLLR6', 'TOLLR7', 'TOLLR8', 'TOLLR9', 'TOLLR10']].values.tolist()\n",
    "combined_df['TOOLBT'] = combined_df[['TOLLB1', 'TOLLB2', 'TOLLB3', 'TOLLB4', 'TOLLB5', 'TOLLB6', 'TOLLB7', 'TOLLB8', 'TOLLB9', 'TOLLB10']].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7873202-907e-44bd-9c38-875c06af7502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SAMPN  PERNO  PLANO  TOTTR  HHMEM  PER1  PER2  PER3  PER4  PER5  \\\n",
      "0        1032036.0    1.0    1.0    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1        1032036.0    1.0    2.0    3.0    2.0   4.0   5.0   NaN   NaN   NaN   \n",
      "2        1032036.0    1.0    3.0    5.0    4.0   2.0   3.0   4.0   5.0   NaN   \n",
      "3        1032036.0    1.0    4.0    2.0    1.0   2.0   NaN   NaN   NaN   NaN   \n",
      "4        1032036.0    1.0    5.0    2.0    1.0   2.0   NaN   NaN   NaN   NaN   \n",
      "...            ...    ...    ...    ...    ...   ...   ...   ...   ...   ...   \n",
      "1210378  7212273.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210379  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210380  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210381  7212324.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1210382  7212388.0    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "         ...  NOGOWHY  Moto_trip    YEAR  VEHT CNTV  \\\n",
      "0        ...      NaN        NaN     NaN   NaN  NaN   \n",
      "1        ...      NaN        NaN     NaN   NaN  NaN   \n",
      "2        ...      NaN        NaN     NaN   NaN  NaN   \n",
      "3        ...      NaN        NaN     NaN   NaN  NaN   \n",
      "4        ...      NaN        NaN     NaN   NaN  NaN   \n",
      "...      ...      ...        ...     ...   ...  ...   \n",
      "1210378  ...      NaN        NaN  2005.0   2.0  2.0   \n",
      "1210379  ...      NaN        NaN  1997.0   2.0  2.0   \n",
      "1210380  ...      NaN        NaN  2004.0   2.0  2.0   \n",
      "1210381  ...      NaN        NaN  2006.0   2.0  2.0   \n",
      "1210382  ...      NaN        NaN  2007.0   2.0  2.0   \n",
      "\n",
      "                                      BUYERT  \\\n",
      "0        [nan, nan, nan, nan, nan, nan, nan]   \n",
      "1        [nan, nan, nan, nan, nan, nan, nan]   \n",
      "2        [nan, nan, nan, nan, nan, nan, nan]   \n",
      "3        [nan, nan, nan, nan, nan, nan, nan]   \n",
      "4        [nan, nan, nan, nan, nan, nan, nan]   \n",
      "...                                      ...   \n",
      "1210378  [nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210379  [nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210380  [nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210381  [nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210382  [nan, nan, nan, nan, nan, nan, nan]   \n",
      "\n",
      "                                           HHNOVT  \\\n",
      "0        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "2        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "3        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "4        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "...                                           ...   \n",
      "1210378  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210379  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210380  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210381  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210382  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "\n",
      "                                           LDPERT  \\\n",
      "0        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "2        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "3        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "4        [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "...                                           ...   \n",
      "1210378  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210379  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210380  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210381  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "1210382  [nan, nan, nan, nan, nan, nan, nan, nan]   \n",
      "\n",
      "                                                    TOOLRT  \\\n",
      "0        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "3        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "...                                                    ...   \n",
      "1210378  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1210379  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1210380  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1210381  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1210382  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                    TOOLBT  \n",
      "0        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "2        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "3        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "4        [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "...                                                    ...  \n",
      "1210378  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1210379  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1210380  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1210381  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1210382  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "\n",
      "[1210383 rows x 193 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop (more than 99% missing values)\n",
    "columns_to_drop = ['O_PRKTY', 'PKAMT', 'PKUNT', 'PRKHW', 'O_PRKHW', 'EMPARK', 'O_TRANSYS', 'CEC', 'BUYER3', 'BUYER4', 'BUYER5', 'BUYER6', 'BUYER7', 'BUYER8', 'O_RESTY', 'O_OWN', 'HHNOV2', 'HHNOV3', 'HHNOV4', 'HHNOV5', 'HHNOV6', 'HHNOV7', 'HHNOV8', 'LDPER2', 'LDPER3', 'LDPER4', 'LDPER5', 'LDPER6', 'LDPER7', 'LDPER8', 'O_LDTPUPR', 'LDINI2', 'LDINI3', 'LDINI4', 'LDINI5', 'LDINI6', 'LDINI7', 'LDINI8', 'LDMODE2', 'LDMODE3', 'LDMODE4', 'LDDPMODE2', 'LDDPMODE3', 'LDDPMODE4', 'LDARMODE2', 'LDARMODE3', 'LDARMODE4', 'RACE3', 'RACE4', 'TPTYP2', 'TPTYP3', 'TPTYP4', 'TPTYP5', 'TPTYP6', 'TPTYP7', 'CLIP2', 'CLIP3', 'COMP', 'MET', 'O_WKSTAT', 'O_INDUS', 'O_OCCUP', 'WXST2_1', 'WXST2_2', 'DTYPE3', 'DTYPE4', 'DTYPE5', 'DTYPE6', 'DTYPE7', 'O_SUBUNT', 'O_SCHOL', 'PRESCH', 'O_PRESCH', 'O_EDUCA', 'TOLLR1', 'TOLLR2', 'TOLLR3', 'TOLLR4', 'TOLLR5', 'TOLLR6', 'TOLLR7', 'TOLLR8', 'TOLLR9', 'TOLLR10', 'TOLLB1', 'TOLLB2', 'TOLLB3', 'TOLLB4', 'TOLLB5', 'TOLLB6', 'TOLLB7', 'TOLLB8', 'TOLLB9', 'TOLLB10', 'NOGOWHY_O', 'InComplete', 'O_BODY', 'FUELT3', 'FUELT4', 'FUELT5', 'FUELT6', 'O_FUELT', 'O_VEHOWN', 'O_VEHDRT', 'O_VEHCYL', 'O_WYCNTV']\n",
    "# of columns dropped: 106\n",
    "\n",
    "# Drop the specified columns\n",
    "df_dropped = combined_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# list of columns to drop (out of scope information)(from investagation) \n",
    "columns_to_drop2 = ['VEHNO', 'O_PRKTY', 'O_TRANSYS', 'TripDistanceFlag', 'AirTripDistance', 'CTFIP', 'TRACT', 'PSTFIP', 'PERWGT', 'EXPPERWGT', 'RECMODE', 'RETMODE', 'STYPE', 'RIBUS', 'TEN', 'PREVCITY', 'PREVSTAT', 'PREVZIP', 'PHLNS', 'RECDate', 'ASSN', 'VEDTE', 'CMPLD', 'HH_Complete', 'HSTFIP', 'MTC_FInalFlag', 'ID', 'TPTYP1', 'TPTYP2', 'TPTYP3', 'TPTYP4', 'TPTYP5', 'TPTYP6', 'TPTYP7', 'O_TPTYP', 'CLIP1', 'CLIP2', 'CLIP3', 'COMP', 'MET', 'PASSTL', 'WNAME', 'WCITY', 'WSTAT', 'WXCORD', 'WYCORD', 'DTYPE1', 'DTYPE2', 'DTYPE3', 'DTYPE4', 'DTYPE5', 'DTYPE6', 'DTYPE7', 'INTRV', 'CMPLG', 'HVLOG', 'WCTFIP', 'WTRACT', 'SCTFIP', 'STRACT', 'WPrimaryCity', 'WSTFIP', 'W2PrimaryCity', 'W2STFIP', 'SPrimaryCity', 'SSTFIP', 'BODY', 'O_BODY', 'FUELT1', 'FUELT2', 'FUELT3', 'FUELT4', 'FUELT5', 'FUELT6', 'O_FUELT', 'CIGLT', 'VEHAQ', 'VEHOWN', 'O_VEHOWN', 'VEHINS', 'VEHOBD', 'VEHTRN', 'VEHDRT', 'O_VEHDRT', 'VEHCYL', 'O_VEHCYL', 'VEHOUT', 'VEHVLT', 'WYCNTV', 'O_WYCNTV','PRESCH', 'O_PRESCH', 'SNAME']\n",
    "df_dropped = df_dropped.drop(columns=columns_to_drop2, errors='ignore')\n",
    "\n",
    "# drop columns that we have merged data for since its in there already.\n",
    "columns_to_drop3 = ['BUYER1', 'BUYER2', 'BUYER3', 'BUYER4', 'BUYER5', 'BUYER6', 'BUYER7', 'BUYER8', 'HHNOV1', 'HHNOV2', 'HHNOV3', 'HHNOV4', 'HHNOV5', 'HHNOV6', 'HHNOV7', 'HHNOV8', 'LDPER1', 'LDPER2', 'LDPER3', 'LDPER4', 'LDPER5', 'LDPER6', 'LDPER7', 'LDPER8', 'TOLLR1', 'TOLLR2', 'TOLLR3', 'TOLLR4', 'TOLLR5', 'TOLLR6', 'TOLLR7', 'TOLLR8', 'TOLLR9', 'TOLLR10', 'TOLLB1', 'TOLLB2', 'TOLLB3', 'TOLLB4', 'TOLLB5', 'TOLLB6', 'TOLLB7', 'TOLLB8', 'TOLLB9', 'TOLLB10']\n",
    "df_dropped = df_dropped.drop(columns=columns_to_drop3, errors='ignore')\n",
    "\n",
    "print(df_dropped)\n",
    "# df_dropped.to_csv('prelimdata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48d283b0-82a9-491e-a145-2b464b5be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#df_dropped.to_csv('prelimdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5462e-0130-44aa-be6c-61b211594f68",
   "metadata": {},
   "source": [
    "## this is for sorting and filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "716f57f3-7ced-4583-a238-f2bb30e92ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# Define the columns of interest\n",
    "# these are school or work indicators \n",
    "columns_of_interest = ['JOBS', 'SCHOL']\n",
    "# Ensure missing values are handled properly some are nana some are none \n",
    "df_dropped[columns_of_interest] = df_dropped[columns_of_interest].replace([np.nan, None], pd.NA)\n",
    "# Identify SAMPN values where at least one column of interest is filled\n",
    "sample_numbers_to_keep = df_dropped[df_dropped[columns_of_interest].notna().any(axis=1)]['SAMPN'].unique()\n",
    "# Ensure compatibility by converting to pandas Series\n",
    "sample_numbers_to_keep_series = pd.Series(sample_numbers_to_keep)\n",
    "\n",
    "\n",
    "# Keep only rows where SAMPN is in the list of sample numbers to keep\n",
    "\n",
    "df_filtered = df_dropped[df_dropped['SAMPN'].isin(sample_numbers_to_keep)]\n",
    "# Step 5: Reset index for clean output (optional)\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb7d3ace-99dc-4194-94b8-8ebadd49578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [SAMPN, PERNO, PLANO, TOTTR, HHMEM, PER1, PER2, PER3, PER4, PER5, NONHH, MODE, DYGOV, PRKTY, PXSTR, PRKMIN, PAYPK, TRANSYS, ROUTE, ACTCNT, ARR_HR, ARR_MIN, DEP_HR, DEP_MIN, TRIPDUR, ACTDUR, PNAME, CITY, STATE, ZIP, TripDistance, PPrimaryCity, TRIPNO, TCF, TCFPERWGT, EXPTCFPERWGT, ACTNO, ACTOTH, ACTHH, ACTOR, ACTWK, ACTSC, ACTRG, ACTFR, ACTOT, APURP, O_APURP, STIME, ETIME, INCEN, ILANG, AREA, STRATA, GTYPE, GFLAG, HHVEH, HHBIC, VEHNEW, RESTY, OWN, INCOM, HHSIZ, NONRELAT, HHEMP, HHSTU, HHLIC, DOW, HTRIPS, HCITY, HSTAT, HZIP, VEHOP, VEHPR, LDTRP, LDFlag, HPFlag, GPS_Complete, HCTRACT, HPrimaryCity, HHWGT, EXPHHWGT, GPS_MTC, LDNO, LDDAT, LDORG, LDOPNM, LDOCITY, LDOST, LDOZIP, LDOCNTR, LDDPNM, LDDCITY, LDDST, LDDZIP, LDDCNTR, LDTPURP, LDWHO, LDMHH, LDINI1, LDMODE1, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 193 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use np.isin() to filter the DataFrame\n",
    "df_filtered2 = df_dropped[np.isin(df_dropped['SAMPN'], sample_numbers_to_keep)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df_filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d44cc-a404-4d11-8159-4f6f526b04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered['SAMPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a6936-c081-42ec-8940-6746d26b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_dropped['SAMPN'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98ea23-3461-40e1-83bf-bbb2e33b3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('studentworker.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9117d77-7403-4b69-aee5-868d37599fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_numbers_to_keep = pd.DataFrame(sample_numbers_to_keep)\n",
    "sample_numbers_to_keep.to_csv('studentworkerSAMPN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2130893-8709-42ca-8884-4d7756e6c3b4",
   "metadata": {},
   "source": [
    "## this is for getting stuff out of highlighted cells dont worry too mich about this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d9458-d69f-43bb-85ef-f289605f4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f57c7-5524-4376-a346-09d7ca7caef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"C:\\Users\\17052\\Downloads\\chts2013_caltrans_raw_survey\\54730190.xlsx\"  # Replace with your file path\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "# Select the active worksheet \n",
    "ws = wb.active  \n",
    "\n",
    "# Define a list to store highlighted cells\n",
    "highlighted_cells = []\n",
    "\n",
    "# Define the target color in ARGB format (here is purple and red)\n",
    "target_colors = {'FF8064A2','FFC00000'}  \n",
    "\n",
    "# List to store highlighted cells in the first column\n",
    "highlighted_cells = []\n",
    "\n",
    "# Loop through each cell in the first column only\n",
    "for row in ws.iter_rows(min_col=1, max_col=1):\n",
    "    cell = row[0]\n",
    "    # Check if the cell has a fill color that matches one of the target colors\n",
    "    if cell.fill and cell.fill.start_color.index in target_colors:\n",
    "        # Append only the cell value to the list if it matches a target color\n",
    "        highlighted_cells.append(cell.value)\n",
    "\n",
    "print(highlighted_cells)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284a849-2467-45b3-881d-3694ef56d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HH_Complete\" in highlighted_cells:\n",
    "    print('greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431ba0d-877c-40d0-872b-9fc5a5c3ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target colors in ARGB format\n",
    "target_colors = {'FFFFFF00'}  # #8064A2 and #FFFF00 in ARGB format\n",
    "\n",
    "# List to store values of highlighted cells in the first column\n",
    "highlighted_values = []\n",
    "\n",
    "# Loop through each cell in the first column only\n",
    "for row in ws.iter_rows(min_col=1, max_col=1):\n",
    "    cell = row[0]\n",
    "    # Check if the cell has a fill color that matches one of the target colors\n",
    "    if cell.fill and cell.fill.start_color.index in target_colors:\n",
    "        # Append only the cell value to the list if it matches a target color\n",
    "        highlighted_values.append(cell.value)\n",
    "print(highlighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db3f82-dd98-4c68-ae4a-6cead388714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target colors in ARGB format\n",
    "target_colors = {'FF92D050'}  # #8064A2 and #FFFF00 in ARGB format\n",
    "\n",
    "# List to store values of highlighted cells in the first column\n",
    "highlighted_values = []\n",
    "\n",
    "# Loop through each cell in the first column only\n",
    "for row in ws.iter_rows(min_col=1, max_col=1):\n",
    "    cell = row[0]\n",
    "    # Check if the cell has a fill color that matches one of the target colors\n",
    "    if cell.fill and cell.fill.start_color.index in target_colors:\n",
    "        # Append only the cell value to the list if it matches a target color\n",
    "        highlighted_values.append(cell.value)\n",
    "\n",
    "print(highlighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc6add-1863-48fe-b705-07be61030163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('raw_data_dic.xlsx')\n",
    "# Select the first two columns\n",
    "first_two_columns_df = df.iloc[:, :2]\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(first_two_columns_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b8a84-c8a0-487d-a33e-1ae581f6d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_list = df_dropped.columns.tolist()\n",
    "\n",
    "\n",
    "df_filtered = first_two_columns_df[first_two_columns_df['Column Name'].isin(column_names_list)]\n",
    "\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a9085-29a1-44af-8e57-659a95f46073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('prelim_dic_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b56943-adc3-40a5-912c-8a9cc41776f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
